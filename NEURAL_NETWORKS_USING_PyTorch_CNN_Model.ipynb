{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyNvrNjOI3BWhUDZkLhoG9NB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkjubran/Fundamentals-of-AI-and-Machine-Learning/blob/main/NEURAL_NETWORKS_USING_PyTorch_CNN_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fitting and Evaluating Convolutional Neural Network using PyTorch\n"
      ],
      "metadata": {
        "id": "mkRHIvM06yZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will demonstrate how to fit and evaluate a ConvolutionalNeural Network (CNN) Model. We will work on the Medical MNIST dataset from Kaggle (https://www.kaggle.com/code/stpeteishii/medical-mnist-6-classify-torchvision-cnn/data).\n",
        "\n",
        "This notebok is based on the code in the notebook https://www.kaggle.com/code/stpeteishii/medical-mnist-6-classify-torchvision-cnn/notebook"
      ],
      "metadata": {
        "id": "1zUsWaQ03Jhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we will start by importing packages and modules necessary for the execution of this notebook."
      ],
      "metadata": {
        "id": "NDLzLZPY_7fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import datasets, transforms, models \n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "h1zbRhoZmTAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Obtain Data"
      ],
      "metadata": {
        "id": "27fY7xYlfKHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clone the dataset Repository**\n",
        "\n",
        "We used a subset of medical images in the original dataset for this notebook. This dataset can be cloned from the GitHub repository https://github.com/mkjubran/MedicalImagingData.git as below"
      ],
      "metadata": {
        "id": "Oe_H_szEfL9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/MedicalImagingData\n",
        "!rm -rf ./AIData\n",
        "!git clone https://github.com/mkjubran/MedicalImagingData.git"
      ],
      "metadata": {
        "id": "1aBK80UEfQr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Data"
      ],
      "metadata": {
        "id": "Qtsmmtk19S27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each class of images is placed in a separate folder in the path '/content/MedicalImagingData/Dataset1/'. The name of the folder is the label of the images. To get the labels, we will read the list of folder names."
      ],
      "metadata": {
        "id": "gRhB2gnamYc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir0='/content/MedicalImagingData/Dataset1/'\n",
        "names0=os.listdir(dir0)\n",
        "print(sorted(names0))\n"
      ],
      "metadata": {
        "id": "UHUGL_BymCJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has about 6000 images. It is important to apply data augmentation techniques to train the CNN model. We will use torchvision.transforms module to alter the data before training the CNN model."
      ],
      "metadata": {
        "id": "dnSOyrOD-ssb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform=transforms.Compose([\n",
        "        transforms.RandomRotation(10),      # rotate +/- 10 degrees\n",
        "        transforms.RandomHorizontalFlip(),  # reverse 50% of images\n",
        "        transforms.Resize(224),             # resize shortest side to 224 pixels\n",
        "        transforms.CenterCrop(224),         # crop longest side to 224 pixels at center\n",
        "        transforms.ToTensor()\n",
        "])\n",
        "\n",
        "''',\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "'''"
      ],
      "metadata": {
        "id": "nklbZaqUmuhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the torchvision.datasets utility classes to build our own datasets."
      ],
      "metadata": {
        "id": "ZpX9sZ_4A8fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=datasets.ImageFolder(root=dir0,transform=train_transform)\n",
        "display(dataset)"
      ],
      "metadata": {
        "id": "2LUfStgGm_fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test the created dataset"
      ],
      "metadata": {
        "id": "bcrGTFNEBTsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names=dataset.classes\n",
        "print(class_names)\n",
        "print(len(class_names))"
      ],
      "metadata": {
        "id": "6_Cyu5IEnPld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will split the data into training and testing datasets."
      ],
      "metadata": {
        "id": "DVbGSKJEBcRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_indices, test_indices = train_test_split(list(range(len(dataset.targets))), test_size=0.2, stratify=dataset.targets)\n",
        "train_data = torch.utils.data.Subset(dataset, train_indices)\n",
        "test_data = torch.utils.data.Subset(dataset, test_indices)\n",
        "\n",
        "print('Size of the dataset = {}'.format(len(dataset.imgs)))\n",
        "print('Size of the training dataset = {} ({}%)'.format(len(train_data.indices), 100*len(train_data.indices)/len(dataset.imgs)))\n",
        "print('Size of the testing dataset = {} ({}%)'.format(len(test_data.indices), 100*len(test_data.indices)/len(dataset.imgs)))"
      ],
      "metadata": {
        "id": "cYS2-EAKnflo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next,we will use Dataloader to feed in the data to the model during training. we wil. use batch size of 100 images."
      ],
      "metadata": {
        "id": "JDQS_-ODCuoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=100\n",
        "train_loader=DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
        "test_loader=DataLoader(test_data,batch_size=batch_size)"
      ],
      "metadata": {
        "id": "tvifcOHinW0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To view the images and the labeles in the train dataloader"
      ],
      "metadata": {
        "id": "jvaMRXoVC-WU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check the 1st batch\n",
        "for images, labels in train_loader:\n",
        "    break\n",
        "\n",
        "print('Label:', labels.numpy()) # here, labels are given as single numbers.\n",
        "print('Class:', *np.array([class_names[i] for i in labels]))\n",
        "\n",
        "im = make_grid(images,nrow=10)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(np.transpose(im.numpy(),(1,2,0)))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B6AvgNiUn9V5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to set the deive to run the deep learning. We would llike to have GPUs but the model can run with low speed on CPUs."
      ],
      "metadata": {
        "id": "xkDTUfUbGkrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print('Using {} device'.format(device))"
      ],
      "metadata": {
        "id": "GGqnL0JWGlek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To build the CNN model using Conved layers."
      ],
      "metadata": {
        "id": "9QVlSWeWEtBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvolutionalNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #Ho=floor[((Hin+2P-d*(K-1))/S)+1], Ho=floor[((224+2*0-1*(3-1))/1)+1]=222, default values; padding=0, dilation=1\n",
        "        self.conv1=nn.Conv2d(in_channels=3,out_channels=6,kernel_size=3,stride=1)\n",
        "        self.conv2=nn.Conv2d(in_channels=6,out_channels=16,kernel_size=3,stride=1)\n",
        "        self.fc1=nn.Linear(in_features=16*54*54,out_features=120) \n",
        "        self.fc2=nn.Linear(in_features=120,out_features=84)\n",
        "        self.fc3=nn.Linear(in_features=84,out_features=20)\n",
        "        self.fc4=nn.Linear(in_features=20,out_features=len(class_names))\n",
        "        \n",
        "    def forward(self,X):\n",
        "        #print(X.shape)  #torch.Size([10, 3, 224, 224]) #which means batch size and image size.\n",
        "        X=F.relu(self.conv1(X))\n",
        "        #print(X.shape) #[100, 6, 222, 222]\n",
        "        X=F.max_pool2d(X,2,2)\n",
        "        #print(X.shape) #[100, 6, 111, 111]\n",
        "        X=F.relu(self.conv2(X))\n",
        "        #print(X.shape) #[100, 16, 109, 109]\n",
        "        X=F.max_pool2d(X,2,2)\n",
        "        #print(X.shape)  #[10, 16, 54, 54]\n",
        "        X=X.view(-1,16*54*54)\n",
        "        #print(X.shape)  #[10, 46656]        \n",
        "        X=F.relu(self.fc1(X))\n",
        "        #print(X.shape)  #[10, 120] \n",
        "        X=F.relu(self.fc2(X))\n",
        "        #print(X.shape)  #[10, 84]\n",
        "        X=F.relu(self.fc3(X))\n",
        "        #print(X.shape)  #[10, 20]\n",
        "        X=self.fc4(X)\n",
        "        #print(X.shape)  #[10, 6]\n",
        "        return F.log_softmax(X,dim=1)\n",
        "\n",
        "model = ConvolutionalNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "Vxzt1FAsp1en"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After defining the model, we need to set the loss function and the optimizer."
      ],
      "metadata": {
        "id": "rf1UnGsgG4xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=0.001)"
      ],
      "metadata": {
        "id": "L1rzOXamp68c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training function including the steps of forward pass and backpropagation"
      ],
      "metadata": {
        "id": "0QUD468IG9lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer, epoch, trn_corr):\n",
        "    batch_size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        batch+=1\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        y_pred = model(X)\n",
        "        loss = criterion(y_pred, y)\n",
        "\n",
        "        predicted=torch.max(y_pred.data,1)[1]\n",
        "        batch_corr=(predicted==y).sum()\n",
        "        trn_corr+=batch_corr\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 5 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"Training: epoch: {epoch} loss: {loss} batch: {batch} accuracy: {trn_corr.item()*100/(current):7.3f}%\")\n",
        "\n",
        "    loss, current = loss.item(), batch * len(X)\n",
        "    print(f\"Training: epoch: {epoch} loss: {loss} batch: {batch} accuracy: {trn_corr.item()*100/(current):7.3f}%\")\n",
        "    return trn_corr, loss"
      ],
      "metadata": {
        "id": "hGpCCDtMHB_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing function to evaluate the model"
      ],
      "metadata": {
        "id": "76ih_A8MJ0sP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn, epoch, tst_corr):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "          batch+=1\n",
        "          X, y = X.to(device), y.to(device)\n",
        "\n",
        "          # Compute prediction error\n",
        "          y_pred=model(X)\n",
        "          loss=criterion(y_pred,y)\n",
        "\n",
        "          predicted=torch.max(y_pred.data,1)[1]\n",
        "          btach_corr=(predicted==y).sum()\n",
        "          tst_corr+=btach_corr\n",
        "\n",
        "    loss, current = loss.item(), batch * len(X)\n",
        "    print(f\"Testing: epoch: {epoch} loss: {loss} accuracy: {tst_corr.item()*100/(current):7.3f}%\")\n",
        "\n",
        "    return tst_corr, loss"
      ],
      "metadata": {
        "id": "ZZMA5P0TJ1Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will train and evaluate the model for 4 epochs while watch the training and testing accuracy."
      ],
      "metadata": {
        "id": "_CDbq_S8MfEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time=time.time()\n",
        "train_losses=[]\n",
        "test_losses=[]\n",
        "train_correct=[]\n",
        "test_correct=[]\n",
        "epochs=4\n",
        "\n",
        "for i in range(epochs):\n",
        "    trn_corr=0\n",
        "    tst_corr=0\n",
        "    \n",
        "    trn_corr, loss = train(train_loader, model, criterion, optimizer, i, trn_corr)\n",
        "    train_losses.append(loss)\n",
        "    train_correct.append(trn_corr)\n",
        "    \n",
        "    tst_corr, loss = test(test_loader, model, criterion, i, tst_corr)\n",
        "    test_losses.append(loss)\n",
        "    test_correct.append(tst_corr)\n",
        "    print(f'-----------------------------------\\n')  \n",
        "\n",
        "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') "
      ],
      "metadata": {
        "id": "EqE8wqXbqHNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Saving the Models"
      ],
      "metadata": {
        "id": "fUcXGIIqMrsw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We wil use PyTorch to save the model"
      ],
      "metadata": {
        "id": "Si3Itu0zNAHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"CNN_model.pth\")\n",
        "print(\"Saved PyTorch Model State to CNN_model.pth\")"
      ],
      "metadata": {
        "id": "8gOnYfqANBy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict New Values Using Models"
      ],
      "metadata": {
        "id": "Ev-Sepbe4apq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us use the model to predict the class and compare it with the ground truth"
      ],
      "metadata": {
        "id": "xr1xabjg6rvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "not ready yet"
      ],
      "metadata": {
        "id": "koxeqaSPNc3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for batch, (X, Y) in enumerate(test_loader):\n",
        "samples_toshow=10\n",
        "batch=0\n",
        "for X, Y in test_loader:\n",
        "        batch+=1\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        x = X[0].to(device)\n",
        "        y = Y[0].to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "          y_pred = model(x)\n",
        "          predicted=torch.max(y_pred.data,1)[1].item()\n",
        "          \n",
        "          print('Sample ', batch)\n",
        "          print('True Label:', y.numpy()) # here, labels are given as single numbers.\n",
        "          print('Predicted Label:', predicted) # here, labels are given as single numbers.\n",
        "\n",
        "          print('True Class:', *np.array([class_names[y]]))\n",
        "          print('Predicted Class:', *np.array([class_names[predicted]]))\n",
        "\n",
        "          im = make_grid(x,nrow=10)\n",
        "          plt.figure(figsize=(4,4))\n",
        "          plt.imshow(np.transpose(im.numpy(),(1,2,0)))\n",
        "\n",
        "          plt.show()\n",
        "          time.sleep(2)\n",
        "\n",
        "        if batch == samples_toshow:\n",
        "          break"
      ],
      "metadata": {
        "id": "eYzYABQDfUwh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
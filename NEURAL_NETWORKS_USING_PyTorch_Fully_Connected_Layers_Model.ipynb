{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "authorship_tag": "ABX9TyMKYnFYrqQeyznzRvSW4iow",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkjubran/Fundamentals-of-AI-and-Machine-Learning/blob/main/NEURAL_NETWORKS_USING_PyTorch_Fully_Connected_Layers_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fitting and Evaluating Multi-layer Perceptron\n"
      ],
      "metadata": {
        "id": "mkRHIvM06yZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will demonstrate how to fit and evaluate a Multi-layer Perceptron (MLP). We will work on a modified version of the cardiovascular dataset from Kaggle (https://www.kaggle.com/code/sulianova/eda-cardiovascular-data/data).\n",
        "\n",
        "Part of this tutorial is based on the code in this link https://github.com/pytorch/tutorials/blob/master/beginner_source/basics/quickstart_tutorial.py"
      ],
      "metadata": {
        "id": "1zUsWaQ03Jhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "27fY7xYlfKHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clone the dataset Repository**\n",
        "\n",
        "The prepared dataset after cleaning, removing outliers, and feature engineering can be cloned from the GitHub repository https://github.com/mkjubran/AIData.git as below"
      ],
      "metadata": {
        "id": "Oe_H_szEfL9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./AIData\n",
        "!git clone https://github.com/mkjubran/AIData.git"
      ],
      "metadata": {
        "id": "1aBK80UEfQr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read the dataset**\n",
        "\n",
        "The data is stored in the cardio_EDA.csv file. Read the input data into a dataframe using the Pandas library (https://pandas.pydata.org/) to read the data."
      ],
      "metadata": {
        "id": "PsR4rC0bfVuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/AIData/cardio_EDA.csv\",sep=\";\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "b4EU2gr-fd4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Display Data Info**\n",
        "\n",
        "Display some information about the dataset using the info() method"
      ],
      "metadata": {
        "id": "yH3Bs7tegOXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "q4ncJdvtgQi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains 53659 records with 15 features for each record. Twelve features are numeric and the rest are objects (strings)."
      ],
      "metadata": {
        "id": "kMLQzWu9gTPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean Data and Remove Outliers"
      ],
      "metadata": {
        "id": "eB7BmZ6NgoCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This data has been processed in previous notebooks\n",
        "- Data Cleaning: https://github.com/mkjubran/Fundamentals-of-AI-and-Machine-Learning/blob/main/EXPLORATORY_DATA_ANALYSIS_%E2%80%93_DATA_CLEANING.ipynb\n",
        "- Feature Selection and Feature Engineering: https://github.com/mkjubran/Fundamentals-of-AI-and-Machine-Learning/blob/main/EXPLORATORY_DATA_ANALYSIS_%E2%80%93_FEATURE_SELECTION_AND_FEATURE_ENGINEERING.ipynb\n",
        "\n",
        "As we noticed from the presented sample of the dataset above some features are highly correlated such as the age and the age_year features. So we need to drop one of these features. Besides, we will drop any not needed features such as the 'id' feature."
      ],
      "metadata": {
        "id": "I6VtkSh0gpgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['id','age'],axis=1, inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "VhDBt-Zlh80m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encode Categorical Data"
      ],
      "metadata": {
        "id": "xjZQyakaiUEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use hot encoding through the get_dummies() method in pandas to encode the data in the 'gender' and 'smoke' features."
      ],
      "metadata": {
        "id": "WsvvhZVUiVtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.get_dummies(df)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "1kLKojV1iyhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember to drop one of the columns that resulted from the hot encoding of each feature. Also, make sure that the original features ('age' and 'smoke') are dropped too."
      ],
      "metadata": {
        "id": "dAiSWbjYi_73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['gender_female','smoke_No'],axis=1,inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "glXLVzQYjUI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Split Data for Training and Testing"
      ],
      "metadata": {
        "id": "hyO6f_ED-Xac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start by specifying the independent variables and the dependent variable. The independent variables are the features that will be used to predict the target feature (class,label). And the dependent variable is the target feature (class, label)."
      ],
      "metadata": {
        "id": "1a-xReXYzMhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# independent variables\n",
        "X=df.drop(['cardio'],axis=1)\n",
        "X.head()\n"
      ],
      "metadata": {
        "id": "8wOyHYkD2JrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dependet variable (target feature, class, label)\n",
        "Y=df.cardio\n",
        "Y.head()"
      ],
      "metadata": {
        "id": "xY4YFIpB2ISz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we will splitting the dataset into training and testing splits, the split ratio is usually 80% training and 20% testing."
      ],
      "metadata": {
        "id": "bhCEG-mWzeq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.2, random_state=200)\n",
        "print('Size of the dataset = {}'.format(len(X)))\n",
        "print('Size of the training dataset = {} ({}%)'.format(len(x_train), 100*len(x_train)/len(X)))\n",
        "print('Size of the testing dataset = {} ({}%)'.format(len(x_test), 100*len(x_test)/len(X)))"
      ],
      "metadata": {
        "id": "dhxAQtJ1z32G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Testing a Fully Connected Network"
      ],
      "metadata": {
        "id": "aCgVJ9KP-KoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import librarier required for PyTorch."
      ],
      "metadata": {
        "id": "KOtuYZpi-QoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor"
      ],
      "metadata": {
        "id": "NY_rbqmQ-hOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the data to be processed using PyTorch. We will use DataLoader to manage data. The DataLoader creates batches from data and we don't have to worry about slicing and shuffling data. We will set the batch size to 64."
      ],
      "metadata": {
        "id": "oIbcB8Qr9Nim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset():\n",
        " \n",
        "  def __init__(self,X,Y):\n",
        "    self.x=X\n",
        "    self.y=Y\n",
        " \n",
        "  def __len__(self):\n",
        "    return len(self.y)\n",
        "   \n",
        "  def __getitem__(self,idx):\n",
        "    return self.x[idx],self.y[idx]\n",
        "    \n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "Xt_train=torch.tensor(x_train.to_numpy(),dtype=torch.float32)\n",
        "y_train_tensor=torch.tensor(y_train.to_numpy(),dtype=torch.float32)\n",
        "Yt_train = y_train_tensor.type(torch.LongTensor)\n",
        "\n",
        "Xt_test=torch.tensor(x_test.to_numpy(),dtype=torch.float32)\n",
        "y_test_tensor=torch.tensor(y_test.to_numpy(),dtype=torch.float32)\n",
        "Yt_test = y_test_tensor.type(torch.LongTensor)\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(MyDataset(Xt_train,Yt_train), batch_size=batch_size)\n",
        "test_dataloader = DataLoader(MyDataset(Xt_test,Yt_test), batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print('Shape of X: {}'.format(X.shape))\n",
        "    print('Shape of y: {} {}'.format(y.shape,y.dtype))\n",
        "    break"
      ],
      "metadata": {
        "id": "7DMDBrAO3ycQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to set the deive to run the deep learning. We would llike to have GPUs but the model can run with low speed on CPUs."
      ],
      "metadata": {
        "id": "7K1Jq5Kp0tTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print('Using {} device'.format(device))"
      ],
      "metadata": {
        "id": "k96m6dF90riu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a model of fully connected layers. To define a neural network in PyTorch, we create a class that inherits from nn.Module."
      ],
      "metadata": {
        "id": "EwjfUgyW0lDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(12, 12),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(12, 12),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(12, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "fSnOmPSk2pB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After defining the model, we need to set the loss function and the optimizer."
      ],
      "metadata": {
        "id": "aDBEUTDm2X2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "bGCmA1aG2yWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training function including the steps of forward pass and backpropagation"
      ],
      "metadata": {
        "id": "BxvgG5_n2g18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "metadata": {
        "id": "caOZlq_T23kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing function to evaluate the model"
      ],
      "metadata": {
        "id": "l3J59jzk3clM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "          X, y = X.to(device), y.to(device)\n",
        "\n",
        "          # Compute prediction error\n",
        "          pred = model(X)\n",
        "          test_loss = loss_fn(pred, y).item()\n",
        "          correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "ZUGz2iu_N2Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will train and evaluate the model for 20 epochs."
      ],
      "metadata": {
        "id": "A0JwEqGs4KoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "1k9XN6a117eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the Models\n",
        "\n",
        "We wil use PyTorch to save the model"
      ],
      "metadata": {
        "id": "eR_239B24VYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print(\"Saved PyTorch Model State to model.pth\")"
      ],
      "metadata": {
        "id": "230n8pv6-te3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict New Values Using Models"
      ],
      "metadata": {
        "id": "Ev-Sepbe4apq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us use the model to predict the class and compare it with the ground truth"
      ],
      "metadata": {
        "id": "xr1xabjg6rvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [0,1]\n",
        "\n",
        "x = Xt_test[10].to(device)\n",
        "y = Yt_test[10].to(device)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pred = model(x)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    print('Predicted: {}, Actual: {}'.format(predicted,actual))"
      ],
      "metadata": {
        "id": "F5iAPP0B-xa_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
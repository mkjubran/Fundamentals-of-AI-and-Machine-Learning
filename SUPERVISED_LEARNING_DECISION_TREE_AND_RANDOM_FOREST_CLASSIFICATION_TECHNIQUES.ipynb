{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "toc_visible": true,
      "authorship_tag": "ABX9TyP05kYj+oywEQDbc7iKOr5B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkjubran/Fundamentals-of-AI-and-Machine-Learning/blob/main/SUPERVISED_LEARNING_DECISION_TREE_AND_RANDOM_FOREST_CLASSIFICATION_TECHNIQUES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SUPERVISED LEARNING - DECISION TREE AND RANDOM FOREST CLASSIFICATION TECHNIQUES\n"
      ],
      "metadata": {
        "id": "mkRHIvM06yZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will demonstrate how to build and evaluate Decision Tree and Random Forest models. We will work on the Heart Failure dataset from Kaggle (https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset)."
      ],
      "metadata": {
        "id": "1zUsWaQ03Jhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "jKIAXwrfe6wC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to import some libraries that will be used during the creation and evaluation of the Decision Tree and Random Forest models."
      ],
      "metadata": {
        "id": "I1fH9Wrse_dw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF3_Cpo16WX8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "27fY7xYlfKHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clone the dataset Repository**\n",
        "\n",
        "The dataset can be cloned from the GitHub repository https://github.com/mkjubran/AIData.git as below"
      ],
      "metadata": {
        "id": "Oe_H_szEfL9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./AIData\n",
        "!git clone https://github.com/mkjubran/AIData.git"
      ],
      "metadata": {
        "id": "1aBK80UEfQr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read the dataset**\n",
        "\n",
        "The data is stored in the heart.csv file. Read the input data into a dataframe using the Pandas library (https://pandas.pydata.org/) to read the data."
      ],
      "metadata": {
        "id": "PsR4rC0bfVuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/AIData/heart.csv\",sep=\",\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "b4EU2gr-fd4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Display Data Info**\n",
        "\n",
        "Display some information about the dataset using the info() method"
      ],
      "metadata": {
        "id": "yH3Bs7tegOXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "q4ncJdvtgQi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains 1025 records with 14 features for each record. All features are numeric."
      ],
      "metadata": {
        "id": "kMLQzWu9gTPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean Data and Remove Outliers"
      ],
      "metadata": {
        "id": "eB7BmZ6NgoCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check Missing Values**\n",
        "\n",
        "Check if there are any missing values in the dataset"
      ],
      "metadata": {
        "id": "I6VtkSh0gpgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "VhDBt-Zlh80m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be observed, no missing data in the dataset."
      ],
      "metadata": {
        "id": "BB_63_j1xsBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove Outliers**\n",
        "\n",
        "Let us get the statistical description of the dataset and check if there is anything not normal"
      ],
      "metadata": {
        "id": "3MYah1QfxvsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "ODjEHd9hxySK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'Age' range is normal between 29 and 77 years. The sex takes two values; 1 = male; 0 = female. The chest pain type (cp) also takes only specific values; 0, 1, 2, and 3. Ideal resting blood pressure (trestbps) is between 90 and 120. Above 140 is considered high and below 90 is considered low. Normal 'chol' is less than 200 mg/dL, borderline high is 200 to 239 mg/dL, and high is at or above 240 mg/dL. A fasting blood Sugar Test of 99 mg/dL or lower is normal, 100 to 125 mg/dL indicates you have prediabetes, and 126 mg/dL or higher indicates you have diabetes. In the dataset the fbs = 1 implies a fasting blood sugar greater than 120 mg/dl, otherwise fbs = 0. Resting electrocardiographic results (restecg) take two values; 1 = detected some heart conditions, 0 did not detect certain heart conditions. thalach is the person’s maximum heart rate achieved. Exercise-induced angina (exang) takes two values; 1 = yes; 0 = no. oldpeak is the ST depression induced by exercise relative to rest. slope is the slope of the peak exercise ST segment — 0: downsloping; 1: flat; 2: upsloping. ca is the number of major vessels and takes four values; 0, 1, 2, and 3. thal is a blood disorder called thalassemia; 0: NULL (dropped from the dataset previously, 1: fixed defect (no blood flow in some part of the heart), 2: normal blood flow, 3: reversible defect (a blood flow is observed but it is not normal). And the target feature 'target' indicates a heart disease; 1 = no, 0= yes.\n",
        "\n",
        "The dataset has no outliers and we leave it to you to check this as explained in a previous session\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ox_T0f86yped"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train And Evaluate Decision Tree Algorithm"
      ],
      "metadata": {
        "id": "WOdszHgjjcmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Decision Tree Algorithm**\n",
        "\n",
        "We will start by specifying the independent variables and the dependent variable. The independent variables are the features that will be used to predict the target feature (class,label). And the dependent variable is the target feature (class, label)."
      ],
      "metadata": {
        "id": "uAVn-ZTbjgSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# independent variables\n",
        "X=df.drop(['target'],axis=1)\n",
        "X.head()"
      ],
      "metadata": {
        "id": "raz1iL76kKAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dependet variable (target feature, class, label)\n",
        "Y=df.target\n",
        "Y.head()"
      ],
      "metadata": {
        "id": "eluEJ2uhkihU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we will splitting the dataset into training and testing splits of the dataset, the split ratio is usually 80% training and 20% testing."
      ],
      "metadata": {
        "id": "7V_n4bF0juln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.2, random_state=200)\n",
        "print('Size of the dataset = {}'.format(len(X)))\n",
        "print('Size of the training dataset = {} ({}%)'.format(len(x_train), 100*len(x_train)/len(X)))\n",
        "print('Size of the testing dataset = {} ({}%)'.format(len(x_test), 100*len(x_test)/len(X)))"
      ],
      "metadata": {
        "id": "V0jKXcncjmcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that we used a random_state so that the results are reproducible. You should avoid setting this argument in your production code so that the split is random at every run.\n",
        "\n",
        "Now, we will import the decision tree model from sklearn and train the model using the training split of the dataset."
      ],
      "metadata": {
        "id": "8F-qHhqtk7J2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "model_dt = tree.DecisionTreeClassifier()\n",
        "model_dt.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "vFjDLuS7lApf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate Decision Tree Model**\n",
        "\n",
        "To evaluate the model, we will compute the training and testing accuracy using the training and testing splits of the dataset"
      ],
      "metadata": {
        "id": "PBpGz_ZRlSus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Acc_train_dt = model_dt.score(x_train, y_train)\n",
        "Acc_test_dt = model_dt.score(x_test, y_test)\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "t = PrettyTable(['Accuracy', 'Decision Tree (%)'])\n",
        "t.add_row(['Training', Acc_train_dt*100])\n",
        "t.add_row(['Testing', Acc_test_dt*100])\n",
        "print(t)"
      ],
      "metadata": {
        "id": "xuDSO5fglkNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train And Evaluate Random Forest Algorithm"
      ],
      "metadata": {
        "id": "n3ZB-g2-eCSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Random Forest Algorithm**\n",
        "\n",
        "We will use the same splits generated dring the training of the Decision Tree model before. So now, we will diectly import the random forest model from sklearn and train the model using the training split of the dataset."
      ],
      "metadata": {
        "id": "WydN3nBUeJTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import ensemble\n",
        "model_rf = ensemble.RandomForestClassifier()\n",
        "model_rf.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "MXwiJebCe3lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate Random Forest Model**\n",
        "\n",
        "To evaluate the model, we will compute the training and testing accuracy using the training and testing splits of the dataset"
      ],
      "metadata": {
        "id": "Yrj1bU3Ifraz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Acc_train_rf = model_rf.score(x_train, y_train)\n",
        "Acc_test_rf = model_rf.score(x_test, y_test)\n",
        "\n",
        "t = PrettyTable(['Accuracy', 'Decision Tree (%)','Random Forest(%)'])\n",
        "t.add_row(['Training', Acc_train_dt*100, Acc_train_rf*100])\n",
        "t.add_row(['Testing', Acc_test_dt*100, Acc_test_rf*100])\n",
        "print(t)"
      ],
      "metadata": {
        "id": "ykqSEEwcfwBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be observed, the training and testing accuracy of the decision tree and the random forest is the same for the current splits of the dataset. The random forest usually achieves better prediction for larger and more complex datasets. We can also compare the performance of both of them with the logistic regressor in the previous sessions as follows"
      ],
      "metadata": {
        "id": "okY9PryOgALH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model\n",
        "logreg = linear_model.LogisticRegression(max_iter=1000)\n",
        "logreg.fit(x_train,y_train)\n",
        "Acc_train_logreg = logreg.score(x_train, y_train)\n",
        "Acc_test_logreg = logreg.score(x_test, y_test)\n",
        "\n",
        "t = PrettyTable(['Accuracy', 'Decision Tree (%)','Random Forest (%)','Logistic (%)'])\n",
        "t.add_row(['Training', Acc_train_dt*100, Acc_train_rf*100,Acc_train_logreg*100])\n",
        "t.add_row(['Testing', Acc_test_dt*100, Acc_test_rf*100, Acc_test_logreg*100])\n",
        "print(t)"
      ],
      "metadata": {
        "id": "kLzcraTpgWB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training and testing accuracy of the random forest is better than the logistic regression."
      ],
      "metadata": {
        "id": "vEA3SD32hGF1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Manual Hyperparameter Tuning**\n",
        "\n",
        "Let us try to fine-tune the model parameters to improve the performance of the random forest model. We will increase the number of decision trees in the algorithm (n_estimators). The default value is 100."
      ],
      "metadata": {
        "id": "GByujFz6DH3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_rf = ensemble.RandomForestClassifier()\n",
        "model_rf.fit(x_train,y_train)\n",
        "Acc_train_rf = model_rf.score(x_train, y_train)\n",
        "Acc_test_rf = model_rf.score(x_test, y_test)\n",
        "\n",
        "model_rf_ne200 = ensemble.RandomForestClassifier(n_estimators=200)\n",
        "model_rf_ne200.fit(x_train,y_train)\n",
        "Acc_train_rf_ne200 = model_rf_ne200.score(x_train, y_train)\n",
        "Acc_test_rf_ne200 = model_rf_ne200.score(x_test, y_test)\n",
        "\n",
        "t = PrettyTable(['Accuracy (RF)', 'n_estimators = 100','n_estimators = 200'])\n",
        "t.add_row(['Training', Acc_train_rf*100, Acc_train_rf_ne200*100])\n",
        "t.add_row(['Testing', Acc_test_rf*100, Acc_test_rf_ne200*100])\n",
        "print(t)"
      ],
      "metadata": {
        "id": "T2YFNGeY9LVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A very small improvement in model accuracy can be achieved. Notice that this is because increasing the number of estimators increases the degree of randomness and thus the improvement. Let us try changing the criterion in the random forest. We will use the 'entropy' while the default value was 'gini'"
      ],
      "metadata": {
        "id": "KWCD3UiiEwsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_rf = ensemble.RandomForestClassifier(random_state=40)\n",
        "model_rf.fit(x_train,y_train)\n",
        "Acc_train_rf = model_rf.score(x_train, y_train)\n",
        "Acc_test_rf = model_rf.score(x_test, y_test)\n",
        "\n",
        "model_rf_entropy = ensemble.RandomForestClassifier(criterion='entropy', random_state=40)\n",
        "model_rf_entropy.fit(x_train,y_train)\n",
        "Acc_train_rf_entropy = model_rf_entropy.score(x_train, y_train)\n",
        "Acc_test_rf_entropy = model_rf_entropy.score(x_test, y_test)\n",
        "\n",
        "t = PrettyTable(['Accuracy (RF)', 'criterion=gini','criterion=entropy'])\n",
        "t.add_row(['Training', Acc_train_rf*100, Acc_train_rf_entropy*100])\n",
        "t.add_row(['Testing', Acc_test_rf*100, Acc_test_rf_entropy*100])\n",
        "print(t)"
      ],
      "metadata": {
        "id": "f8UaqfsW-UU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No or almost no improvement is achieved in this case. You can also try feature scaling and/or normalization, oversampling in case of class imbalance."
      ],
      "metadata": {
        "id": "Opkvrx3JF72J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving and Loading Models"
      ],
      "metadata": {
        "id": "_HEZ9o_0YFum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the joblib method from sklearn library (https://scikit-learn.org/stable/modules/model_persistence.html) to save and load the models. To save the model we use the dump method as"
      ],
      "metadata": {
        "id": "bTo0mTb5YHfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib as jb\n",
        "jb.dump(model_rf, './Model_rf.joblib')"
      ],
      "metadata": {
        "id": "xmTG3qJaYQ6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And to load the rained logistic model, we will use the load() method"
      ],
      "metadata": {
        "id": "DhMNaUh0ZRBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_rf_joblib = jb.load('./Model_rf.joblib')"
      ],
      "metadata": {
        "id": "UXpXcTnWZV88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict New Values Using Models"
      ],
      "metadata": {
        "id": "hR9JhQwVZdud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To predict the target values for new data, we will use the loaded model"
      ],
      "metadata": {
        "id": "3mf1mk6KZhfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.head()"
      ],
      "metadata": {
        "id": "zvnkzBIiZ1R7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict = model_rf_joblib.predict(x_test)\n",
        "dfnew=x_test.copy()\n",
        "dfnew['target_predict']=y_predict"
      ],
      "metadata": {
        "id": "s9kPkcqbZ9R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the test split, we have the actual value of the 'cardio', so we can add it to the new dataframe for comparison purposes."
      ],
      "metadata": {
        "id": "FYcwiwo6aVy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfnew['target_actual']=y_test\n",
        "dfnew.head()"
      ],
      "metadata": {
        "id": "jxyScT03aWXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the measured accuracy above, the cardio_predict and cardio_acutal should match in ~96% (testing accuracy) of the records. Below are the miscalssified records."
      ],
      "metadata": {
        "id": "alwQnVpUapXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfnew[dfnew['target_predict'] != dfnew['target_actual']]"
      ],
      "metadata": {
        "id": "wRDqOh4BnOQK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
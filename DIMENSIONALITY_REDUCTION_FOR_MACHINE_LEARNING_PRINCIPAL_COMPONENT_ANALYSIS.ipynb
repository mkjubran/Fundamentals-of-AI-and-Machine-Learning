{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "toc_visible": true,
      "authorship_tag": "ABX9TyPpXrj994yfR3Ikp9WwcwiT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkjubran/Fundamentals-of-AI-and-Machine-Learning/blob/main/DIMENSIONALITY_REDUCTION_FOR_MACHINE_LEARNING_PRINCIPAL_COMPONENT_ANALYSIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DIMENSIONALITY REDUCTION FOR MACHINE LEARNING - PRINCIPAL COMPONENT ANALYSIS\n"
      ],
      "metadata": {
        "id": "mkRHIvM06yZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will demonstrate how to use dimensionality reduction before training and evaluating Random Forest models. We will use the Principle Component Analysis (PCA) algorithm for dimensionality reduction. We will work on the Heart Failure dataset from Kaggle (https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset)."
      ],
      "metadata": {
        "id": "1zUsWaQ03Jhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "jKIAXwrfe6wC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to import some libraries that will be used during the creation and evaluation of the Random Forest model."
      ],
      "metadata": {
        "id": "I1fH9Wrse_dw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF3_Cpo16WX8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "27fY7xYlfKHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clone the dataset Repository**\n",
        "\n",
        "The prepared dataset after cleaning, removing outliers, and feature engineering can be cloned from the GitHub repository https://github.com/mkjubran/AIData.git as below"
      ],
      "metadata": {
        "id": "Oe_H_szEfL9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./AIData\n",
        "!git clone https://github.com/mkjubran/AIData.git"
      ],
      "metadata": {
        "id": "1aBK80UEfQr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read the dataset**\n",
        "\n",
        "The data is stored in the cardio_EDA.csv file. Read the input data into a dataframe using the Pandas library (https://pandas.pydata.org/) to read the data."
      ],
      "metadata": {
        "id": "PsR4rC0bfVuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/AIData/cardio_EDA.csv\",sep=\";\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "b4EU2gr-fd4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Display Data Info**\n",
        "\n",
        "Display some information about the dataset using the info() method"
      ],
      "metadata": {
        "id": "yH3Bs7tegOXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "q4ncJdvtgQi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains 53659 records with 15 features for each record. Twelve features are numeric and the rest are objects (strings)."
      ],
      "metadata": {
        "id": "kMLQzWu9gTPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean Data and Remove Outliers"
      ],
      "metadata": {
        "id": "eB7BmZ6NgoCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This data has been processed in previous notebooks\n",
        "- Data Cleaning: https://github.com/mkjubran/Fundamentals-of-AI-and-Machine-Learning/blob/main/EXPLORATORY_DATA_ANALYSIS_%E2%80%93_DATA_CLEANING.ipynb\n",
        "- Feature Selection and Feature Engineering: https://github.com/mkjubran/Fundamentals-of-AI-and-Machine-Learning/blob/main/EXPLORATORY_DATA_ANALYSIS_%E2%80%93_FEATURE_SELECTION_AND_FEATURE_ENGINEERING.ipynb\n",
        "\n",
        "As we noticed from the presented sample of the dataset above some features are highly correlated such as the age and the age_year features. To demonstrate the effectiveness of PCA on Random Forest, we will keep these two features. But we will drop any not needed features such as the 'id' feature."
      ],
      "metadata": {
        "id": "I6VtkSh0gpgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['id'],axis=1, inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "VhDBt-Zlh80m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encode Categorical Data"
      ],
      "metadata": {
        "id": "xjZQyakaiUEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use hot encoding through the get_dummies() method in pandas to encode the data in the 'gender' and 'smoke' features."
      ],
      "metadata": {
        "id": "WsvvhZVUiVtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.get_dummies(df)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "1kLKojV1iyhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember to drop one of the columns that resulted from the hot encoding of each feature. Also, make sure that the original features ('age' and 'smoke') are dropped too."
      ],
      "metadata": {
        "id": "dAiSWbjYi_73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['gender_female','smoke_No'],axis=1,inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "glXLVzQYjUI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train And Evaluate Random Forest Classifier"
      ],
      "metadata": {
        "id": "WOdszHgjjcmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Random Forest Classifier**\n",
        "\n",
        "We will start by specifying the independent variables and the dependent variable. The independent variables are the features that will be used to predict the target feature (class,label). And the dependent variable is the target feature (class, label)."
      ],
      "metadata": {
        "id": "uAVn-ZTbjgSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# independent variables\n",
        "X=df.drop(['cardio'],axis=1)\n",
        "X.head()"
      ],
      "metadata": {
        "id": "raz1iL76kKAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dependet variable (target feature, class, label)\n",
        "Y=df.cardio\n",
        "Y.head()"
      ],
      "metadata": {
        "id": "eluEJ2uhkihU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is known that the Random Forest does not perform well when features are monotonic transformations of other features such as the 'age' and 'age_years' (this makes the trees of the forest less independent from each other). So we will use PCA to get rid of collinear features. (all collinear features will end up in a single PCA component).\n",
        "\n",
        "Now, we will reduce the dimensionality of the features dataframe by transforming the features using PCA"
      ],
      "metadata": {
        "id": "Xl-PVylWo1QA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=13)\n",
        "pca.fit(X)\n",
        "X_PCA13 = pca.transform(X)\n",
        "X_PCA13=pd.DataFrame(X_PCA13)"
      ],
      "metadata": {
        "id": "qOus_usdpmXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display a snapshot of the PCA dataframe"
      ],
      "metadata": {
        "id": "RL_L6Bt5DJOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_PCA13.head()"
      ],
      "metadata": {
        "id": "b9r9IekvDKDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now use cross-validation to train and evaluate the Random Forest classifier. We will use the same parameters obtained through the automated hyperparameter tuning (grid search) in the notebook https://github.com/mkjubran/Fundamentals-of-AI-and-Machine-Learning/blob/main/PERFORMANCE_OF_MACHINE_LEARNING_SYSTEMS_%E2%80%93_K_FOLD_CROSS_VALIDATION.ipynb  "
      ],
      "metadata": {
        "id": "PFXtnQR6NHht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn import ensemble\n",
        "model_rf = ensemble.RandomForestClassifier(max_features=3,max_samples=2000,n_estimators=200)\n",
        "cv_value = 10\n",
        "score_rf_PCA13 = cross_validate(model_rf,X_PCA13,Y,cv = cv_value, return_train_score=True)\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "t = PrettyTable(['PCA Components', 'train_score','test_score'])\n",
        "t.add_row(['All PCA', score_rf_PCA13['train_score'].mean(),score_rf_PCA13['test_score'].mean()])\n",
        "print(t)\n",
        "\n"
      ],
      "metadata": {
        "id": "3U4r7prGNM1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us reduce the dimensionality of the input features by dropping the last PCA component (component no. 12)"
      ],
      "metadata": {
        "id": "ylyFxLcZO_O8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_PCA12=X_PCA13.drop([12], axis=1)\n",
        "X_PCA12.head()"
      ],
      "metadata": {
        "id": "90d1YCYeO16u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After dropping the last PCA component and reducing the dimensionality to 12, let us train and evaluate the random forest classifier using the reduced input features"
      ],
      "metadata": {
        "id": "1nRWGsSpX99P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score_rf_PCA12 = cross_validate(model_rf,X_PCA12,Y,cv = cv_value, return_train_score=True)\n",
        "\n",
        "t = PrettyTable(['PCA Components', 'train_score','test_score'])\n",
        "t.add_row(['All PCA (13)', score_rf_PCA13['train_score'].mean(),score_rf_PCA13['test_score'].mean()])\n",
        "\n",
        "t.add_row(['First 12', score_rf_PCA12['train_score'].mean(),score_rf_PCA12['test_score'].mean()])\n",
        "print(t)"
      ],
      "metadata": {
        "id": "DM_QU1XHYfJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be observed the dropped PCA component (last component number 12) has a negligible effect on the training and testing accuracy of the random forest. Let's drop one more PCA component (Number 11) and check the results."
      ],
      "metadata": {
        "id": "VTd0wvgxckdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_PCA11=X_PCA13.drop([11,12], axis=1)\n",
        "score_rf_PCA11 = cross_validate(model_rf,X_PCA11,Y,cv = cv_value, return_train_score=True)\n",
        "\n",
        "t = PrettyTable(['PCA Components','train_score','test_score'])\n",
        "t.add_row(['All PCA (13)', score_rf_PCA13['train_score'].mean(),score_rf_PCA13['test_score'].mean()])\n",
        "\n",
        "t.add_row(['First 12', score_rf_PCA12['train_score'].mean(),score_rf_PCA12['test_score'].mean()])\n",
        "\n",
        "t.add_row(['First 11', score_rf_PCA11['train_score'].mean(),score_rf_PCA11['test_score'].mean()])\n",
        "print(t)"
      ],
      "metadata": {
        "id": "MhdG4Tq0cNwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the results show that dropping the last two PCA components has negligible effect on the training and testing accuracy of the random forest. Let's keep only the firt drop one more PCA component (Number 11) and check the results."
      ],
      "metadata": {
        "id": "HTtAWyphbPkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_PCA10=X_PCA13.drop([10,11,12], axis=1)\n",
        "score_rf_PCA10 = cross_validate(model_rf,X_PCA10,Y,cv = cv_value, return_train_score=True)\n",
        "\n",
        "X_PCA9=X_PCA13.drop([9,10,11,12], axis=1)\n",
        "score_rf_PCA9 = cross_validate(model_rf,X_PCA9,Y,cv = cv_value, return_train_score=True)\n",
        "\n",
        "X_PCA7=X_PCA13.drop([7,8,9,10,11,12], axis=1)\n",
        "score_rf_PCA7 = cross_validate(model_rf,X_PCA7,Y,cv = cv_value, return_train_score=True)\n",
        "\n",
        "X_PCA5=X_PCA13.drop([5,6,7,8,9,10,11,12], axis=1)\n",
        "score_rf_PCA5 = cross_validate(model_rf,X_PCA5,Y,cv = cv_value, return_train_score=True)\n",
        "\n",
        "X_PCA3=X_PCA13.drop([3,4,5,6,7,8,9,10,11,12], axis=1)\n",
        "score_rf_PCA3 = cross_validate(model_rf,X_PCA3,Y,cv = cv_value, return_train_score=True)\n",
        "\n",
        "t = PrettyTable(['PCA Components', 'train_score','test_score'])\n",
        "t.add_row(['All PCA (13)',score_rf_PCA13['train_score'].mean(),score_rf_PCA13['test_score'].mean()])\n",
        "\n",
        "t.add_row(['First 12', score_rf_PCA12['train_score'].mean(),score_rf_PCA12['test_score'].mean()])\n",
        "\n",
        "t.add_row(['First 11', score_rf_PCA11['train_score'].mean(),score_rf_PCA11['test_score'].mean()])\n",
        "\n",
        "t.add_row(['First 10', score_rf_PCA10['train_score'].mean(),score_rf_PCA10['test_score'].mean()])\n",
        "\n",
        "t.add_row(['First 9', score_rf_PCA9['train_score'].mean(),score_rf_PCA9['test_score'].mean()])\n",
        "\n",
        "t.add_row(['First 7', score_rf_PCA7['train_score'].mean(),score_rf_PCA7['test_score'].mean()])\n",
        "\n",
        "t.add_row(['First 5', score_rf_PCA5['train_score'].mean(),score_rf_PCA5['test_score'].mean()])\n",
        "\n",
        "t.add_row(['First 3', score_rf_PCA3['train_score'].mean(),score_rf_PCA3['test_score'].mean()])\n",
        "\n",
        "print(t)"
      ],
      "metadata": {
        "id": "pATFv6B9c9vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can observe, reducing the dimensionality of model input results in a negligible effect on the accuracy performance. The number of components that we need to keep for model fitting depends on the dataset and the model used. In our case we will keep the first 7 components."
      ],
      "metadata": {
        "id": "kBhrxsM2fpQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After we decided what components to keep, we split the daaset and fit the model using the PCA components of the training split."
      ],
      "metadata": {
        "id": "GQUwbENDHBVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.2, random_state=200)\n",
        "\n",
        "pca.fit(x_train)\n",
        "x_train_PCA = pca.transform(x_train)\n",
        "x_train_PCA = pd.DataFrame(x_train_PCA)\n",
        "x_train_PCA.drop([7,8,9,10,11,12],axis=1, inplace=True)\n",
        "model_rf = ensemble.RandomForestClassifier(max_features=3,max_samples=2000,n_estimators=200)\n",
        "model_rf.fit(x_train_PCA,y_train)"
      ],
      "metadata": {
        "id": "q3S5BvtyHVRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving and Loading Models"
      ],
      "metadata": {
        "id": "_HEZ9o_0YFum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the joblib method from sklearn library (https://scikit-learn.org/stable/modules/model_persistence.html) to save and load the models. To save the model we use the dump method as"
      ],
      "metadata": {
        "id": "bTo0mTb5YHfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib as jb\n",
        "jb.dump(model_rf, './Model_rf.joblib')\n",
        "\n",
        "jb.dump(pca, './Model_pca.joblib')"
      ],
      "metadata": {
        "id": "xmTG3qJaYQ6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And to load the trained random forest model, we will use the load() method"
      ],
      "metadata": {
        "id": "DhMNaUh0ZRBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_rf_joblib = jb.load('./Model_rf.joblib')\n",
        "model_pca_joblib = jb.load('./Model_pca.joblib')"
      ],
      "metadata": {
        "id": "UXpXcTnWZV88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict New Values Using Models"
      ],
      "metadata": {
        "id": "hR9JhQwVZdud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To predict the target values for new data, we will use the loaded model"
      ],
      "metadata": {
        "id": "3mf1mk6KZhfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.head()"
      ],
      "metadata": {
        "id": "zvnkzBIiZ1R7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply PCA of the test fetures using the loaded model"
      ],
      "metadata": {
        "id": "ZWijoHBqjhUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_PCA = model_pca_joblib.transform(x_test)\n",
        "x_test_PCA = pd.DataFrame(x_test_PCA)"
      ],
      "metadata": {
        "id": "XX8-zH5ejm4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we keep drop the last 6 PCA components as we did when we trained the model."
      ],
      "metadata": {
        "id": "-X5qv9dZkDf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_PCA.drop([7,8,9,10,11,12],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "5E6Um14pkNXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, to predict the label, we apply the remaining PCA components to the Random Forest classifier"
      ],
      "metadata": {
        "id": "4F8moA-smFNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict = model_rf_joblib.predict(x_test_PCA)\n",
        "dfnew=x_test.copy()\n",
        "dfnew['cardio_predict']=y_predict"
      ],
      "metadata": {
        "id": "s9kPkcqbZ9R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the test split, we have the actual value of the 'cardio', so we can add it to the new dataframe for comparison purposes."
      ],
      "metadata": {
        "id": "FYcwiwo6aVy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfnew['cardio_actual']=y_test\n",
        "dfnew.head()"
      ],
      "metadata": {
        "id": "jxyScT03aWXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the measured accuracy above, the cardio_predict and cardio_acutal should match in ~70% (testing accuracy) of the records."
      ],
      "metadata": {
        "id": "alwQnVpUapXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfnew[dfnew['cardio_predict'] != dfnew['cardio_actual']]"
      ],
      "metadata": {
        "id": "wRDqOh4BnOQK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
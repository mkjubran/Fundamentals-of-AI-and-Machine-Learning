{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "toc_visible": true,
      "authorship_tag": "ABX9TyNyI8tbbwwNJ5ViIYvRQel+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkjubran/Fundamentals-of-AI-and-Machine-Learning/blob/main/PERFORMANCE_OF_MACHINE_LEARNING_SYSTEMS_%E2%80%93_K_FOLD_CROSS_VALIDATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PERFORMANCE OF MACHINE LEARNING SYSTEMS â€“ K-FOLD CROSS-VALIDATION\n"
      ],
      "metadata": {
        "id": "mkRHIvM06yZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will demonstrate how to use the K-Fold cross validation to evaluate Random Forest models. We will work on a modified version of the cardiovascular dataset from Kaggle (https://www.kaggle.com/code/sulianova/eda-cardiovascular-data/data)."
      ],
      "metadata": {
        "id": "1zUsWaQ03Jhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "jKIAXwrfe6wC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to import some libraries that will be used during the creation and evaluation of the Random Forest model."
      ],
      "metadata": {
        "id": "I1fH9Wrse_dw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF3_Cpo16WX8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "27fY7xYlfKHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clone the dataset Repository**\n",
        "\n",
        "The prepared dataset after cleaning, removing outliers, and feature engineering can be cloned from the GitHub repository https://github.com/mkjubran/AIData.git as below"
      ],
      "metadata": {
        "id": "Oe_H_szEfL9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./AIData\n",
        "!git clone https://github.com/mkjubran/AIData.git"
      ],
      "metadata": {
        "id": "1aBK80UEfQr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read the dataset**\n",
        "\n",
        "The data is stored in the cardio_EDA.csv file. Read the input data into a dataframe using the Pandas library (https://pandas.pydata.org/) to read the data."
      ],
      "metadata": {
        "id": "PsR4rC0bfVuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/AIData/cardio_EDA.csv\",sep=\";\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "b4EU2gr-fd4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Display Data Info**\n",
        "\n",
        "Display some information about the dataset using the info() method"
      ],
      "metadata": {
        "id": "yH3Bs7tegOXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "q4ncJdvtgQi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains 53659 records with 14 features for each record. Twelve features are numeric and the rest are objects (strings)."
      ],
      "metadata": {
        "id": "kMLQzWu9gTPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean Data and Remove Outliers"
      ],
      "metadata": {
        "id": "eB7BmZ6NgoCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This data has been processed in previous notebooks\n",
        "- Data Cleaning: https://github.com/mkjubran/Fundamentals-of-AI-and-Machine-Learning/blob/main/EXPLORATORY_DATA_ANALYSIS_%E2%80%93_DATA_CLEANING.ipynb\n",
        "- Feature Selection and Feature Engineering: https://github.com/mkjubran/Fundamentals-of-AI-and-Machine-Learning/blob/main/EXPLORATORY_DATA_ANALYSIS_%E2%80%93_FEATURE_SELECTION_AND_FEATURE_ENGINEERING.ipynb\n",
        "\n",
        "As we noticed from the presented sample of the dataset above some features are highly correlated such as the age and the age_year features. So we need to drop one of these features. Besides, we will drop any not needed features such as the 'id' feature."
      ],
      "metadata": {
        "id": "I6VtkSh0gpgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['id','age'],axis=1, inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "VhDBt-Zlh80m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encode Categorical Data"
      ],
      "metadata": {
        "id": "xjZQyakaiUEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use hot encoding through the get_dummies() method in pandas to encode the data in the 'gender' and 'smoke' features."
      ],
      "metadata": {
        "id": "WsvvhZVUiVtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.get_dummies(df)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "1kLKojV1iyhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember to drop one of the columns that resulted from the hot encoding of each feature. Also, make sure that the original features ('age' and 'smoke') are dropped too."
      ],
      "metadata": {
        "id": "dAiSWbjYi_73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['gender_female','smoke_No'],axis=1,inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "glXLVzQYjUI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train And Evaluate Random Forest Classifier"
      ],
      "metadata": {
        "id": "WOdszHgjjcmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Random Forest Classifier**\n",
        "\n",
        "We will start by specifying the independent variables and the dependent variable. The independent variables are the features that will be used to predict the target feature (class,label). And the dependent variable is the target feature (class, label)."
      ],
      "metadata": {
        "id": "uAVn-ZTbjgSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# independent variables\n",
        "X=df.drop(['cardio'],axis=1)\n",
        "X.head()"
      ],
      "metadata": {
        "id": "raz1iL76kKAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dependet variable (target feature, class, label)\n",
        "Y=df.cardio\n",
        "Y.head()"
      ],
      "metadata": {
        "id": "eluEJ2uhkihU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we will splitting the dataset into training and testing splits of the dataset, the split ratio is usually 80% training and 20% testing."
      ],
      "metadata": {
        "id": "7V_n4bF0juln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.2, random_state=200)\n",
        "print('Size of the dataset = {}'.format(len(X)))\n",
        "print('Size of the training dataset = {} ({}%)'.format(len(x_train), 100*len(x_train)/len(X)))\n",
        "print('Size of the testing dataset = {} ({}%)'.format(len(x_test), 100*len(x_test)/len(X)))"
      ],
      "metadata": {
        "id": "V0jKXcncjmcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that we used a random_state so that the results are reproducible. You should avoid setting this argument in your production code so that the split is random at every run.\n",
        "\n",
        "Now, we will import the random forest model from sklearn and train the model using the training split of the dataset."
      ],
      "metadata": {
        "id": "8F-qHhqtk7J2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import ensemble\n",
        "model_rf = ensemble.RandomForestClassifier()\n",
        "model_rf.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "MXwiJebCe3lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate Random Forest Model**\n",
        "\n",
        "To evaluate the model, we will compute the training and testing accuracy using the training and testing splits of the dataset"
      ],
      "metadata": {
        "id": "Yrj1bU3Ifraz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Acc_train_rf = model_rf.score(x_train, y_train)\n",
        "Acc_test_rf = model_rf.score(x_test, y_test)\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "t = PrettyTable(['Accuracy', 'Random Forest(%)'])\n",
        "t.add_row(['Training', Acc_train_rf*100])\n",
        "t.add_row(['Testing', Acc_test_rf*100])\n",
        "print(t)"
      ],
      "metadata": {
        "id": "ykqSEEwcfwBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, the results change with the change in the split of data between training and testing splits. Try running the code below several times and see how the value of the accuracy change."
      ],
      "metadata": {
        "id": "oLa4ovNXAAkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.2)\n",
        "model_rf = ensemble.RandomForestClassifier()\n",
        "model_rf.fit(x_train,y_train)\n",
        "Acc_train_rf = model_rf.score(x_train, y_train)\n",
        "Acc_test_rf = model_rf.score(x_test, y_test)\n",
        "\n",
        "t = PrettyTable(['Accuracy', 'Random Forest(%)'])\n",
        "t.add_row(['Training', Acc_train_rf*100])\n",
        "t.add_row(['Testing', Acc_test_rf*100])\n",
        "print(t)"
      ],
      "metadata": {
        "id": "B_DaasojAPaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So what are the correct value of training and testing accuracy? To resolve this we use the Cross-Validation method."
      ],
      "metadata": {
        "id": "8_0v6y4QAqEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "cv_value = 10\n",
        "score_rf = cross_validate(model_rf,X,Y,cv = cv_value, return_train_score=True)"
      ],
      "metadata": {
        "id": "X7N23DPBCerD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The average performance measures of the model are"
      ],
      "metadata": {
        "id": "s_6ptRNpF5jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('fit_time = {}'.format(score_rf['fit_time'].mean()))\n",
        "print('score_time = {}'.format(score_rf['score_time'].mean()))\n",
        "print('train_score = {}'.format(score_rf['train_score'].mean()))\n",
        "print('test_score = {}'.format(score_rf['test_score'].mean()))"
      ],
      "metadata": {
        "id": "ekOzfV97EoQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manual Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "pYwev0AG_NG4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us try to fine-tune the model parameters to improve the performance of the random forest model. We will do this without cross-validation. Let us try increasing the number of decision trees in the algorithm (n_estimators). The default value is 100."
      ],
      "metadata": {
        "id": "GByujFz6DH3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_rf = ensemble.RandomForestClassifier()\n",
        "model_rf.fit(x_train,y_train)\n",
        "Acc_train_rf = model_rf.score(x_train, y_train)\n",
        "Acc_test_rf = model_rf.score(x_test, y_test)\n",
        "\n",
        "model_rf_ne200 = ensemble.RandomForestClassifier(n_estimators=50)\n",
        "model_rf_ne200.fit(x_train,y_train)\n",
        "Acc_train_rf_ne200 = model_rf_ne200.score(x_train, y_train)\n",
        "Acc_test_rf_ne200 = model_rf_ne200.score(x_test, y_test)\n",
        "\n",
        "t = PrettyTable(['Accuracy (RF)', 'n_estimators = 100','n_estimators = 200'])\n",
        "t.add_row(['Training', Acc_train_rf*100, Acc_train_rf_ne200*100])\n",
        "t.add_row(['Testing', Acc_test_rf*100, Acc_test_rf_ne200*100])\n",
        "print(t)"
      ],
      "metadata": {
        "id": "T2YFNGeY9LVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A very small improvement in model accuracy can be achieved. Notice that this is because increasing the number of estimators increases the degree of randomness and thus the improvement. Let us try changing the criterion in the random forest. We will use the 'entropy' while the default value was 'gini'"
      ],
      "metadata": {
        "id": "KWCD3UiiEwsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_rf = ensemble.RandomForestClassifier(random_state=40)\n",
        "model_rf.fit(x_train,y_train)\n",
        "Acc_train_rf = model_rf.score(x_train, y_train)\n",
        "Acc_test_rf = model_rf.score(x_test, y_test)\n",
        "\n",
        "model_rf_entropy = ensemble.RandomForestClassifier(criterion='entropy', random_state=40)\n",
        "model_rf_entropy.fit(x_train,y_train)\n",
        "Acc_train_rf_entropy = model_rf_entropy.score(x_train, y_train)\n",
        "Acc_test_rf_entropy = model_rf_entropy.score(x_test, y_test)\n",
        "\n",
        "t = PrettyTable(['Accuracy (RF)', 'criterion=gini','criterion=entropy'])\n",
        "t.add_row(['Training', Acc_train_rf*100, Acc_train_rf_entropy*100])\n",
        "t.add_row(['Testing', Acc_test_rf*100, Acc_test_rf_entropy*100])\n",
        "print(t)"
      ],
      "metadata": {
        "id": "f8UaqfsW-UU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we achieved small or no improvement in accuracy. \n",
        "\n",
        "It seems that the model suffers from overfitting because the training accuracy is much higher than the testing accuracy. So let us try to gain some improvement in testing accuracy by optimizing the parameters related to overfitting such as the number of features to consider when looking for the best split (max_features) and the number of samples to draw from training data split to train each base estimator (max_samples). We will start by tuning max_features. Possible values are 2, 3, 4, ... 12."
      ],
      "metadata": {
        "id": "PpHf6meO0ZJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for max_features in range(2,12,1):\n",
        "   model_rf = ensemble.RandomForestClassifier(max_features=max_features)\n",
        "   model_rf.fit(x_train,y_train)\n",
        "   Acc_train_rf = model_rf.score(x_train, y_train)\n",
        "   Acc_test_rf = model_rf.score(x_test, y_test)\n",
        "   print('max_features = {}, Acc_train_rf = {}, Acc_test_rf = {}'.format(max_features,Acc_train_rf,Acc_test_rf))"
      ],
      "metadata": {
        "id": "G1wejyem0Zzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the maximum testing accuracy is achieved when max_features is 2 or 3. The number of records in x_train is 42927, so let us try different values for the max_samples."
      ],
      "metadata": {
        "id": "AbUb_s2g2Ou_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for max_samples in range(1000,20000,1000):\n",
        "    model_rf = ensemble.RandomForestClassifier(max_features=3,max_samples=max_samples,n_estimators=200)\n",
        "    model_rf.fit(x_train,y_train)\n",
        "    Acc_train_rf = model_rf.score(x_train, y_train)\n",
        "    Acc_test_rf = model_rf.score(x_test, y_test)\n",
        "    print('max_samples = {}, Acc_train_rf = {}, Acc_test_rf = {}'.format(max_samples,Acc_train_rf,Acc_test_rf))"
      ],
      "metadata": {
        "id": "m2QjvmBB2y-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automate Hyperparameter Tuning with Cross-validation "
      ],
      "metadata": {
        "id": "Kbk5dumb_Xvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Grid Search with Cross Validation**\n",
        "\n",
        "Instead of the manual search for tuning the classifier parameters with the cross-validation, we can use the GridSearchCV to automate the tuning of parameters."
      ],
      "metadata": {
        "id": "BNfJjMws7sNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#default cv value is 5\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = {'max_features':range(2,8,1),'max_samples':range(1000,10000,1000),'n_estimators':[100,200]}\n",
        "model_rf = ensemble.RandomForestClassifier()\n",
        "clf = GridSearchCV(model_rf, parameters)\n",
        "clf.fit(x_train, y_train)\n",
        "clf.best_params_"
      ],
      "metadata": {
        "id": "luCaOqKj5gyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After we decided on the best parameter values, we again fit the model using these parameters."
      ],
      "metadata": {
        "id": "GQUwbENDHBVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_rf = ensemble.RandomForestClassifier('max_features'=2,'max_samples'=2000,'n_estimators'=200)\n",
        "model_rf.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "q3S5BvtyHVRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving and Loading Models"
      ],
      "metadata": {
        "id": "_HEZ9o_0YFum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the joblib method from sklearn library (https://scikit-learn.org/stable/modules/model_persistence.html) to save and load the models. To save the model we use the dump method as"
      ],
      "metadata": {
        "id": "bTo0mTb5YHfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib as jb\n",
        "jb.dump(model_rf, './Model_rf.joblib')"
      ],
      "metadata": {
        "id": "xmTG3qJaYQ6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And to load the trained random forest model, we will use the load() method"
      ],
      "metadata": {
        "id": "DhMNaUh0ZRBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_rf_joblib = jb.load('./Model_rf.joblib')"
      ],
      "metadata": {
        "id": "UXpXcTnWZV88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict New Values Using Models"
      ],
      "metadata": {
        "id": "hR9JhQwVZdud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To predict the target values for new data, we will use the loaded model"
      ],
      "metadata": {
        "id": "3mf1mk6KZhfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.head()"
      ],
      "metadata": {
        "id": "zvnkzBIiZ1R7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict = model_rf_joblib.predict(x_test)\n",
        "dfnew=x_test.copy()\n",
        "dfnew['cardio_predict']=y_predict"
      ],
      "metadata": {
        "id": "s9kPkcqbZ9R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the test split, we have the actual value of the 'cardio', so we can add it to the new dataframe for comparison purposes."
      ],
      "metadata": {
        "id": "FYcwiwo6aVy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfnew['cardio_actual']=y_test\n",
        "dfnew.head()"
      ],
      "metadata": {
        "id": "jxyScT03aWXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the measured accuracy above, the cardio_predict and cardio_acutal should match in ~97% (testing accuracy) of the records."
      ],
      "metadata": {
        "id": "alwQnVpUapXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfnew[dfnew['cardio_predict'] != dfnew['cardio_actual']]"
      ],
      "metadata": {
        "id": "wRDqOh4BnOQK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
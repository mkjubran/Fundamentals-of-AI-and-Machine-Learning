{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "toc_visible": true,
      "authorship_tag": "ABX9TyNqOZXPTXFzTvDZEdDa+dpC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkjubran/Fundamentals-of-AI-and-Machine-Learning/blob/main/UNSUPERVISED_LEARNING_K_MEANS_CLUSTERING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UNSUPERVISED LEARNING - K-MEANS CLUSTERING\n"
      ],
      "metadata": {
        "id": "mkRHIvM06yZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will demonstrate how to build and evaluate K-means clustering as an example of an unsupervised learning technique. We will work on a modified version of the cardiovascular dataset from Kaggle (https://www.kaggle.com/code/sulianova/eda-cardiovascular-data/data). The aim of this problem is to cluster the patients into two groups with common characteristics."
      ],
      "metadata": {
        "id": "1zUsWaQ03Jhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "jKIAXwrfe6wC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to import some libraries that will be used during the creation and evaluation of K-means clustering models."
      ],
      "metadata": {
        "id": "I1fH9Wrse_dw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF3_Cpo16WX8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "from sklearn.cluster import KMeans\n",
        "#warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "27fY7xYlfKHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clone the dataset Repository**\n",
        "\n",
        "The prepared dataset after cleaning, removing outliers, and feature engineering can be cloned from the GitHub repository https://github.com/mkjubran/AIData.git as below"
      ],
      "metadata": {
        "id": "Oe_H_szEfL9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./AIData\n",
        "!git clone https://github.com/mkjubran/AIData.git"
      ],
      "metadata": {
        "id": "1aBK80UEfQr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read the dataset**\n",
        "\n",
        "The data is stored in the cardio_EDA.csv file. Read the input data into a dataframe using the Pandas library (https://pandas.pydata.org/) to read the data."
      ],
      "metadata": {
        "id": "PsR4rC0bfVuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/AIData/cardio_EDA.csv\",sep=\";\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "b4EU2gr-fd4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Display Data Info**\n",
        "\n",
        "Display some information about the dataset using the info() method"
      ],
      "metadata": {
        "id": "yH3Bs7tegOXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "q4ncJdvtgQi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains 53659 records with 14 features for each record. Twelve features are numeric and the rest are objects (strings)."
      ],
      "metadata": {
        "id": "kMLQzWu9gTPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean Data and Remove Outliers"
      ],
      "metadata": {
        "id": "eB7BmZ6NgoCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This data has been processed in previous notebooks\n",
        "- Data Cleaning: https://github.com/mkjubran/Fundamentals-of-AI-and-Machine-Learning/blob/main/EXPLORATORY_DATA_ANALYSIS_%E2%80%93_DATA_CLEANING.ipynb\n",
        "- Feature Selection and Feature Engineering: https://github.com/mkjubran/Fundamentals-of-AI-and-Machine-Learning/blob/main/EXPLORATORY_DATA_ANALYSIS_%E2%80%93_FEATURE_SELECTION_AND_FEATURE_ENGINEERING.ipynb\n",
        "\n",
        "As we noticed from the presented sample of the dataset above some features are highly correlated such as the age and the age_year features. So we need to drop one of these features. Besides, we will drop any not needed features such as the 'id' feature."
      ],
      "metadata": {
        "id": "I6VtkSh0gpgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['id','age'],axis=1, inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "VhDBt-Zlh80m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encode Categorical Data"
      ],
      "metadata": {
        "id": "xjZQyakaiUEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use one hot encoding through the get_dummies() method in pandas to encode the data in the 'gender' and 'smoke' features."
      ],
      "metadata": {
        "id": "WsvvhZVUiVtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.get_dummies(df)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "1kLKojV1iyhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember to drop one of the columns that resulted from the hot encoding of each feature. Also, make sure that the original features ('age' and 'smoke') are dropped too."
      ],
      "metadata": {
        "id": "dAiSWbjYi_73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['gender_female','smoke_No'],axis=1,inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "glXLVzQYjUI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train And Evaluate K-means clustering"
      ],
      "metadata": {
        "id": "WOdszHgjjcmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scaling/Normalizing Features**\n",
        "\n",
        "In the beginning, we need to scale/normalize all features within the same range. Here, we will use the MinMaxScaler from sklearn as below"
      ],
      "metadata": {
        "id": "WqehThaOO8FX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range = (0,1))\n",
        "\n",
        "x=df.copy()\n",
        "scaler.fit(x)\n",
        "x_normalized = scaler.transform(x)"
      ],
      "metadata": {
        "id": "BjmNneKZO8jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train K-means clustering**\n",
        "\n",
        "We will splitting the dataset into training and testing splits of the dataset, the split ratio is usually 80% training and 20% testing."
      ],
      "metadata": {
        "id": "7V_n4bF0juln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "km = KMeans( n_clusters = 10 )\n",
        "km.fit_predict( x_normalized )\n"
      ],
      "metadata": {
        "id": "V0jKXcncjmcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The KMeans cluster centers are also computed while fit and predict steps. However, it will be difficult to draw them in the features domian (10 features and so need to view in 10 dimensions)"
      ],
      "metadata": {
        "id": "wJl4N80pOeXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "km_cluster_centers_ = km.cluster_centers_\n",
        "print(km_cluster_centers_)"
      ],
      "metadata": {
        "id": "xxq1qLCwOp47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate K-means clustering**\n",
        "\n",
        "The sum of the square error (SSE) is used to evaluate the quality of clustering. This SSE is the inertia_ attribute of the sklearn K-means algorithm. A low value means records within every cluster are very close to its center."
      ],
      "metadata": {
        "id": "7IjDoKXqQdFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "km.inertia_"
      ],
      "metadata": {
        "id": "ivn0NO0jQuCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Number of Clusters**\n",
        "\n",
        "To determine the best number of clusters, we will measure the SSE for different number-of-cluster values, this should be a decreasing curve. Then, we will choose the number-of-cluster value just before the SSE flatten. This process will take some time depending on the minimum number of clusters and the maximum number of clusters allowed. You are recommended to use a high step size when the range of search is large, and then to reduce the range and reduce the step size until you determine the best number-of-clusters value.\n",
        "\n",
        "We will start with minimum number of clusters equal to 2, and maximum number of clusters equal to 102, and step size 10."
      ],
      "metadata": {
        "id": "bs2nw84mQ_qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SSE=[]\n",
        "MinClusters=2\n",
        "MaxClusters=102\n",
        "StepClusters=10\n",
        "for k in range(MinClusters,MaxClusters,StepClusters):\n",
        "    km = KMeans(n_clusters=k)\n",
        "    km.fit_predict(x)\n",
        "    km.inertia_\n",
        "    SSE.append([k,km.inertia_])\n",
        "    print([k,km.inertia_])"
      ],
      "metadata": {
        "id": "Iksl5P8qRO9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us plot the number of clusters Vs inertia curve"
      ],
      "metadata": {
        "id": "dM94dhAiSSeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "SSE=np.array(SSE)\n",
        "plt.plot(SSE[:,0],SSE[:,1],color = 'red')\n",
        "plt.scatter(SSE[:,0],SSE[:,1],color = 'blue')"
      ],
      "metadata": {
        "id": "cI3k0CnrSYVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be observed, the curve starts to flatten arround the number of clusters equal to 20. So, we will refine the search such that the minimum number of clusters equals 2, the maximum number of clusters equals 36, and the step size is 2."
      ],
      "metadata": {
        "id": "Pv_jRPo6c-WE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SSE=[]\n",
        "MinClusters=2\n",
        "MaxClusters=35\n",
        "StepClusters=2\n",
        "for k in range(MinClusters,MaxClusters,StepClusters):\n",
        "    km = KMeans(n_clusters=k)\n",
        "    km.fit_predict(x)\n",
        "    km.inertia_\n",
        "    SSE.append([k,km.inertia_])\n",
        "    print([k,km.inertia_])\n",
        "\n",
        "SSE=np.array(SSE)\n",
        "plt.plot(SSE[:,0],SSE[:,1],color = 'red')\n",
        "plt.scatter(SSE[:,0],SSE[:,1],color = 'blue')"
      ],
      "metadata": {
        "id": "gjVXg71cdmFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is called the Elbow method to determine the value of number-of-clusters. \n",
        "\n",
        "Another approach to determine the best number-of-clusters value is by using the silhouette metric. Silhouette score tells how far away the datapoints in one cluster are, from the datapoints in another cluster. The range of silhouette score is from -1 to 1. Score should be closer to 1 than -1."
      ],
      "metadata": {
        "id": "IBX8opwbg-rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "MinClusters=2\n",
        "MaxClusters=35\n",
        "StepClusters=2\n",
        "for k in range(MinClusters,MaxClusters,StepClusters):\n",
        "    km = KMeans(n_clusters=k)\n",
        "    cluster_labels = km.fit_predict(x)\n",
        "    silhouette_avg = silhouette_score(x, cluster_labels)\n",
        "    print(\"For n_clusters =\", k,\n",
        "          \"The average silhouette_score is :\", silhouette_avg)"
      ],
      "metadata": {
        "id": "KDD9g6-9g9Q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the best number of clusters according to both methods is 6."
      ],
      "metadata": {
        "id": "Ub1ZWwOTkAPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 6\n",
        "km = KMeans(n_clusters=k)\n",
        "km.fit_predict(x)"
      ],
      "metadata": {
        "id": "SFIT_SWnj-Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving and Loading Models"
      ],
      "metadata": {
        "id": "_HEZ9o_0YFum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the joblib method from sklearn library (https://scikit-learn.org/stable/modules/model_persistence.html) to save and load the models. To save the model we use the dump method as"
      ],
      "metadata": {
        "id": "bTo0mTb5YHfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib as jb\n",
        "jb.dump(km, './Model_km.joblib')"
      ],
      "metadata": {
        "id": "xmTG3qJaYQ6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And to load the K-means model, we will use the load() method"
      ],
      "metadata": {
        "id": "DhMNaUh0ZRBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "km_joblib = jb.load('./Model_km.joblib')"
      ],
      "metadata": {
        "id": "UXpXcTnWZV88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cluster Data Using K-means Models"
      ],
      "metadata": {
        "id": "hR9JhQwVZdud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To predict the target values for new data, we will use the loaded model"
      ],
      "metadata": {
        "id": "3mf1mk6KZhfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict = km_joblib.predict(x)\n",
        "dfnew=x.copy()\n",
        "dfnew['Class']=y_predict\n",
        "dfnew.head()"
      ],
      "metadata": {
        "id": "s9kPkcqbZ9R5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}